# Neural Network From Scratch

This project is a simple implementation of a neural network built from scratch using Python and NumPy, without using high-level machine learning libraries such as TensorFlow or PyTorch.

The purpose of this project is to understand how neural networks work internally rather than treating them as black-box models.

---

## What this project does

- Implements a basic Artificial Neural Network (ANN)
- Performs forward propagation to generate predictions
- Uses backpropagation to compute gradients
- Trains the model using gradient descent
- Uses the sigmoid activation function
- Demonstrates learning on a simple binary classification task

---

## Why I built this

Instead of only learning neural networks theoretically or by directly using libraries, I wanted to understand how models actually learn.

By implementing the network from scratch, I was able to:
- Understand how data flows through layers
- See how errors are calculated
- Learn how weights are updated step by step
- Bridge the gap between theory and implementation

---

## Technologies Used

- Python
- NumPy

(No high-level ML frameworks were used)

---

ðŸ“ˆ Learning Outcomes

Through this project, I gained a clear understanding of:

Forward and backward propagation

Gradient-based optimization

Model training behavior

How neural networks learn from data


This project strengthened my foundation in machine learning and helped me move beyond surface-level implementation.


---

ðŸ”® Future Improvements

Add support for multiple layers

Experiment with different activation functions

Add loss visualization

Extend to multiclass classification



----

ðŸ“Œ Note

This project focuses on model logic and learning behavior, not deployment.
The emphasis is on understanding core concepts through implementation.


----
